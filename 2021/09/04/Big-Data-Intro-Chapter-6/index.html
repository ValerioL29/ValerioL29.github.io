

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="Ken hexo blog">
  <meta name="author" content="Jiacheng Li">
  <meta name="keywords" content="">
  
  <title>Big Data Intro - Chapter 6 - Elapsed</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.11","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Elapsed</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/bg.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="Big Data Intro - Chapter 6">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-09-04 13:30" pubdate>
        2021年9月4日 下午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      4k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      44
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Big Data Intro - Chapter 6</h1>
            
            <div class="markdown-body">
              <h1 id="第06章-Spark基于内存的分布式计算"><a href="#第06章-Spark基于内存的分布式计算" class="headerlink" title="第06章 Spark基于内存的分布式计算"></a>第06章 Spark基于内存的分布式计算</h1><h2 id="Part-1-Spark-概述"><a href="#Part-1-Spark-概述" class="headerlink" title="Part 1 Spark 概述"></a>Part 1 Spark 概述</h2><h3 id="Spark-简介"><a href="#Spark-简介" class="headerlink" title="Spark 简介"></a>Spark 简介</h3><p><strong>Apache Spark</strong> 是一种基于<strong>内存的快速、通用、可扩展的</strong>大数据计算引擎。</p>
<p>同时，<strong>Spark</strong> 是一站式解决方案，集批处理、实时流处理、交互式查询、图计算与机器学习于一体。</p>
<h3 id="Spark-应用场景"><a href="#Spark-应用场景" class="headerlink" title="Spark 应用场景"></a>Spark 应用场景</h3><ul>
<li><strong>批处理</strong>可用于 <strong>ETL（抽取、转换、加载）</strong></li>
<li><strong>机器学习</strong>可用于自动判断电商的买家评论是好评还是差评</li>
<li><strong>交互式分析</strong>可用于查询 Hive 数据仓库</li>
<li><strong>流处理</strong>可用于页面点击流分析，推荐系统，舆情分析等实时业务<ul>
<li>舆情分析：就是对互联网敏感、热点信息进行监控、分析处理</li>
</ul>
</li>
</ul>
<blockquote>
<p>需要反复操作的次数越多，所需读取的数据量越大，受益越大，因为数据已经在内存了</p>
</blockquote>
<h3 id="Spark-特点"><a href="#Spark-特点" class="headerlink" title="Spark 特点"></a>Spark 特点</h3><p><strong>Spark</strong> 用十分之一的资源，便可获得 3 倍于 <strong>MapReduce</strong> 的性能</p>
<h4 id="轻"><a href="#轻" class="headerlink" title="轻"></a>轻</h4><p><strong>Spark 核心代码只有 3 万行</strong></p>
<ul>
<li>Scala 语言的简洁和丰富表达力</li>
<li>巧妙利用了 Hadoop 和 Mesos 的基础设施</li>
</ul>
<h4 id="快"><a href="#快" class="headerlink" title="快"></a>快</h4><p><strong>Spark 对小数据集可达到亚秒级的延迟</strong></p>
<ul>
<li>对大数据集的迭代机器学习即席查询、图计算等应用，<strong>Spark</strong> 版本比基于 <strong>MapReduce、Hive 和 Pregel</strong> 的实现快</li>
<li>内存计算、数据本地性和传输优化、调度优化</li>
</ul>
<h4 id="灵"><a href="#灵" class="headerlink" title="灵"></a>灵</h4><p><strong>Spark 提供了不同层面的灵活性</strong></p>
<ul>
<li>Scala 语言 trait 动态混入策略（如可更换的集群调度器、序列化库）</li>
<li>允许扩展新的数据算子、新的数据源、新的 <strong>language bindings</strong></li>
<li><strong>Spark</strong> 支持内存计算、多迭代批量处理、即席查询、流处理和图计算等多种范式</li>
</ul>
<h4 id="巧"><a href="#巧" class="headerlink" title="巧"></a>巧</h4><p><strong>巧妙借力现有大数据组件</strong></p>
<ul>
<li><strong>Spark</strong> 借 <strong>Hadoop</strong> 之势，与 <strong>Hadoop</strong> 无缝结合</li>
<li>图计算借用 <strong>Pregel 和 PowerGraph</strong> 的 API 以及 PowerGraph 的点分割思想</li>
</ul>
<h2 id="Part-2-Spark-数据结构"><a href="#Part-2-Spark-数据结构" class="headerlink" title="Part 2 Spark 数据结构"></a>Part 2 Spark 数据结构</h2><h3 id="Spark-核心概念-RDD"><a href="#Spark-核心概念-RDD" class="headerlink" title="Spark 核心概念 RDD"></a>Spark 核心概念 RDD</h3><p><img src="https://alipicbed.oss-cn-beijing.aliyuncs.com/img/image-20210904104454432.png" srcset="/img/loading.gif" lazyload alt="RDD 在 Spark 中的角色"></p>
<p><strong>RDD（Resilient Distributed Datasets）</strong> 即<strong>弹性分布式数据集</strong>，是一个<strong>只读的，可分区的</strong>分布式数据集</p>
<ul>
<li>RDD 默认存储在<strong>内存</strong>，当内存不足时，溢写到磁盘</li>
<li>RDD 数据以<strong>分区的形式</strong>在集群中存储</li>
<li>RDD 具有<strong>血统机制（Lineage）</strong>，发生数据丢失时，可快速进行数据恢复</li>
</ul>
<p>可以说，<strong>RDD</strong> 是 <strong>Spark</strong> 对基础数据的抽象</p>
<ul>
<li><strong>RDD 的生成：</strong><ul>
<li>从 Hadoop 文件系统（或与 Hadoop 兼容的其它存储系统）输入创建（如 HDFS）</li>
<li>从父RDD转换得到新的RDD</li>
</ul>
</li>
<li><strong>RDD 的存储：</strong><ul>
<li>用户可以选择不同的存储级别存储 RDD 以便重用（11种）</li>
<li>RDD 默认存储于内存，但当内存不足时，RDD会溢出到磁盘中</li>
</ul>
</li>
<li><strong>RDD 的分区：</strong><ul>
<li>为减少网络传输代价，和进行分布式计算，需对RDD进行分区</li>
<li>在需要进行分区时会根据每条记录 Key 进行分区，以此保证两个数据集能高效进行 Join 操作</li>
</ul>
</li>
<li><strong>RDD 的优点：</strong><ul>
<li>RDD是只读的，静态的。因此可提供更高的容错能力</li>
<li>可以实现推测式执行</li>
</ul>
</li>
</ul>
<h3 id="RDD-的依赖关系"><a href="#RDD-的依赖关系" class="headerlink" title="RDD 的依赖关系"></a>RDD 的依赖关系</h3><p><img src="https://alipicbed.oss-cn-beijing.aliyuncs.com/img/image-20210904104551904.png" srcset="/img/loading.gif" lazyload alt="ND 和 WD"></p>
<h4 id="Dependency（依赖）"><a href="#Dependency（依赖）" class="headerlink" title="Dependency（依赖）"></a>Dependency（依赖）</h4><ul>
<li>窄依赖是指父 RDD 的<strong>每个分区最多被一个</strong>子 RDD 的一个<strong>分区（即 Partition）</strong>所用</li>
<li>宽依赖是指父 RDD 的<strong>每个分区对应一个</strong>子 RDD 的多个分区，是 stage 划分的依据</li>
</ul>
<h4 id="Lineage（血统）：依赖的链条"><a href="#Lineage（血统）：依赖的链条" class="headerlink" title="Lineage（血统）：依赖的链条"></a>Lineage（血统）：依赖的链条</h4><ul>
<li>RDD 数据集通过 <strong>Lineage</strong> 记住了它是如何从其他 RDD 中演变过来的</li>
</ul>
<h3 id="宽窄依赖的区别"><a href="#宽窄依赖的区别" class="headerlink" title="宽窄依赖的区别"></a>宽窄依赖的区别</h3><h4 id="算子"><a href="#算子" class="headerlink" title="算子"></a>算子</h4><p><strong>窄依赖</strong>指的是每一个父 RDD 的 Partition 最多被子 RDD 的一个 Partition 使用</p>
<ul>
<li>如 <code>map</code>、<code>filter</code>，<code>union</code></li>
</ul>
<p><strong>宽依赖</strong>指的是多个子 RDD 的 Partition 会依赖同一个父 RDD 的 Partition</p>
<ul>
<li>如 <code>groupByKey</code>，<code>reduceByKey</code>，<code>sortByKey</code></li>
</ul>
<h4 id="容错性"><a href="#容错性" class="headerlink" title="容错性"></a>容错性</h4><p>假设某个节点出现了故障</p>
<ul>
<li>窄依赖：只需要重算与<strong>子 RDD 分区对应的父 RDD 分区</strong>即可</li>
<li>宽依赖：极端情况下，<strong>所有的父 RDD 分区都要重新计算</strong></li>
</ul>
<p>如下图所示，b1 分区丢失，则需要重新计算 a1, a2 和 a3</p>
<p><img src="https://alipicbed.oss-cn-beijing.aliyuncs.com/img/image-20210904105710231.png" srcset="/img/loading.gif" lazyload alt="宽依赖故障情况"></p>
<h4 id="传输"><a href="#传输" class="headerlink" title="传输"></a>传输</h4><p>宽依赖往往对应着 <strong>Shuffle</strong> 操作，需要在运行过程中将同一个父 RDD 的分区传入到不同的子 RDD 分区中，中间可能涉及多个节点之间的数据传输。</p>
<p>窄依赖的每个父 RDD 的分区只会传入到一个子 RDD 分区中，通常可以在一个节点内完成转换。</p>
<h3 id="RDD-的-Stage-划分"><a href="#RDD-的-Stage-划分" class="headerlink" title="RDD 的 Stage 划分"></a>RDD 的 Stage 划分</h3><p><img src="https://alipicbed.oss-cn-beijing.aliyuncs.com/img/image-20210904110026807.png" srcset="/img/loading.gif" lazyload alt="RDD 的 Stage 划分"></p>
<p>实际应用提交的 Job 中 RDD 依赖关系是十分复杂的，依据这些依赖关系来划分 stage 自然 是十分困难的，Spark 此时就利用了前文提到的依赖关系</p>
<ul>
<li>调度器从 DAG 图末端出发，逆向遍历整个依赖关系链</li>
<li>遇到 <strong>ShuffleDependency（宽依赖关系的一种叫法）</strong>就断开</li>
<li>遇到 <strong>NarrowDependency</strong> 就将其加入到当前 stage</li>
</ul>
<p>stage 中task 数目由 stage 末端的 RDD分区个数来决定，RDD 转换是基于分区的一种<strong>粗粒度计算</strong>，一个 stage 执行的结果就是这几个分区构成的 RDD</p>
<h3 id="RDD-操作类型"><a href="#RDD-操作类型" class="headerlink" title="RDD 操作类型"></a>RDD 操作类型</h3><p><strong>Spark</strong> 中的操作大致可以分为创建操作、转换操作、控制操作和行为操作</p>
<ul>
<li><strong>创建操作（Creation Operation）：</strong>用于 RDD 创建工作。RDD 创建只有两种方法<ul>
<li>一种是来自于内存集合和外部存储系统</li>
<li>另一种是通过转换操作生成的 RDD</li>
</ul>
</li>
<li><strong>转换操作（Transformation Operation）：</strong>将 RDD 通过一定的操作转变成新的 RDD，RDD 的转换操作是<strong>惰性操作</strong>，他只是定义了一个新的 RDD，并没有立即执行</li>
<li><strong>控制操作（Control Operation）：</strong>进行 RDD 持久化，可以让 RDD 按不同的存储策略保存在磁盘或者内存中，比如 cache 接口默认将 RDD 缓存在内存中</li>
<li><strong>行动操作（Action Operation）：</strong>能够出发 <strong>Spark</strong> 运行的操作，其分为两类——<ul>
<li>一类操作输出计算结果</li>
<li>另一类将 RDD 保存到外部文件系统或者数据库中</li>
</ul>
</li>
</ul>
<p>下面详细介绍这四类操作</p>
<h4 id="创建操作"><a href="#创建操作" class="headerlink" title="创建操作"></a>创建操作</h4><p>目前有两张类型的基础 RDD：</p>
<ul>
<li>并行集合：接受一个已经存在的集合，然后进行并行计算</li>
<li>外部存储：在一个文件的每条记录上运行函数<ul>
<li>只要文件系统是 HDFS，或者 Hadoop 支持的任意存储系统即可</li>
</ul>
</li>
</ul>
<p>这两种类型的 RDD 都可以通过相同的方式进行操作，从而获得子 RDD 等一系列拓展，形成血统关系图。</p>
<h4 id="控制操作"><a href="#控制操作" class="headerlink" title="控制操作"></a>控制操作</h4><p>Spark 可以将 RDD <strong>持久化</strong>到内存或磁盘文件系统中，把 RDD 持久化到内存中<strong>可以极大地提高迭代计算以及个计算模型之间的数据共享</strong>。</p>
<p>一般情况下执行节点 60% 内存用于缓存数据，剩下的 40% 用于运行任务。</p>
<p>Spark 中使用 persist 和 cache 操作进行持久化，其中 cache 是 persist() 的特例。</p>
<h4 id="转换操作-Transformation-算子"><a href="#转换操作-Transformation-算子" class="headerlink" title="转换操作 - Transformation 算子"></a>转换操作 - Transformation 算子</h4><p><img src="https://alipicbed.oss-cn-beijing.aliyuncs.com/img/image-20210904111602403.png" srcset="/img/loading.gif" lazyload alt="Transformation 算子"></p>
<h4 id="行动操作-Action-算子"><a href="#行动操作-Action-算子" class="headerlink" title="行动操作 - Action 算子"></a>行动操作 - Action 算子</h4><p><img src="https://alipicbed.oss-cn-beijing.aliyuncs.com/img/image-20210904111625270.png" srcset="/img/loading.gif" lazyload alt="Action 算子"></p>
<h3 id="DataFrame-概念"><a href="#DataFrame-概念" class="headerlink" title="DataFrame 概念"></a>DataFrame 概念</h3><p>与 RDD 类似，<strong>DataFrame</strong> 也是一个<strong>不可变弹性分布式数据集</strong>。除了数据以外，还记录数据的<strong>结构信息</strong>，即 <strong>Schema</strong>，类似二维表格</p>
<p><strong>DataFrame</strong> 的查询计划可以通过 <strong>Spark Catalyst Optimiser</strong> 进行优化，即使 Spark 经验并不丰富，用 <strong>DataFrame</strong> 写的程序也可以尽量被转化为<strong>高效的形式</strong>予以执行</p>
<blockquote>
<p><strong>Catalyst工作流程：</strong></p>
<ul>
<li>SQL 语句首先通过 Parser 模块被解析为语法树，此棵树称为 Unresolved Logical Plan</li>
<li>Unresolved Logical Plan 通过 Analyzer 模块借助于数据元数据解析为 Logical Plan</li>
<li>此时再通过各种基于规则的优化策略进行深入优化，得到 Optimized Logical Plan</li>
<li>优化后的逻辑执行计划依然是逻辑的，并不能被 Spark 系统理解，此时需要将此逻辑执行计划转换为Physical Plan</li>
</ul>
</blockquote>
<h3 id="DataSet-概念"><a href="#DataSet-概念" class="headerlink" title="DataSet 概念"></a>DataSet 概念</h3><p><strong>DataFrame</strong> 是 <strong>DataSet</strong> 的特例，**DataFrame = DataSet[Row]**，所以可以通过 <code>as</code> 方法将 DataFrame 转换为 DataSet。</p>
<p><strong>Row</strong> 是一个通用的类型，所有的表结构信息都用 Row 来表示。</p>
<p><strong>DataSet</strong> 是强类型的，可以有 <strong>DataSet[Car]，DataSet[Person]</strong></p>
<h3 id="DataFrame、DataSet、RDD-表现形式的区别"><a href="#DataFrame、DataSet、RDD-表现形式的区别" class="headerlink" title="DataFrame、DataSet、RDD 表现形式的区别"></a>DataFrame、DataSet、RDD 表现形式的区别</h3><p>假设 RDD 中的两行数据长这样</p>
<p><img src="https://alipicbed.oss-cn-beijing.aliyuncs.com/img/image-20210904112524752.png" srcset="/img/loading.gif" lazyload alt="RDD 示例"></p>
<p>那么在 DataFrame 中的数据长这样</p>
<p><img src="https://alipicbed.oss-cn-beijing.aliyuncs.com/img/image-20210904112559381.png" srcset="/img/loading.gif" lazyload alt="DataFrame 示例"></p>
<p>那么 DataSet 中的数据长这样</p>
<p><img src="https://alipicbed.oss-cn-beijing.aliyuncs.com/img/image-20210904112647391.png" srcset="/img/loading.gif" lazyload alt="DataSet 示例"></p>
<p>由上可见——</p>
<h4 id="DataFrame-与-DataSet-的区别"><a href="#DataFrame-与-DataSet-的区别" class="headerlink" title="DataFrame 与 DataSet 的区别"></a>DataFrame 与 DataSet 的区别</h4><ul>
<li><strong>DataFrame</strong><ul>
<li>DataFrame 每一行的类型固定为 Row，只有通过解析才能获取各个字段的值， 每一列的值没法直接访问</li>
<li>DataFrame 编译器缺少类型安全检查</li>
</ul>
</li>
<li><strong>DataSet</strong><ul>
<li>每一行是什么类型是不一定的，可以是 Person，也可以是 Row</li>
<li>DataSet 类型安全</li>
</ul>
</li>
</ul>
<h4 id="RDD-与-DataFrame-DataSet的区别"><a href="#RDD-与-DataFrame-DataSet的区别" class="headerlink" title="RDD 与 DataFrame / DataSet的区别"></a>RDD 与 DataFrame / DataSet的区别</h4><ul>
<li><strong>RDD</strong><ul>
<li>用于 Spark1.X 各模块的 API</li>
<li>不支持 SparkSQL 操作</li>
<li>不支持代码自动优化</li>
</ul>
</li>
<li><strong>DataFrame 与 DataSet</strong><ul>
<li>用于 Spark2.X 各模块的 API</li>
<li>支持 SparkSQL 操作，还能注册临时表，进行 SQL 语句操作</li>
<li>支持一些方便的保存方式，比如保存成 csv，json 等格式</li>
<li>基于 SparkSQL 引擎构建，支持代码自动优化</li>
</ul>
</li>
</ul>
<h4 id="RDD-与-DataFrame、DataSet-三者的共性"><a href="#RDD-与-DataFrame、DataSet-三者的共性" class="headerlink" title="RDD 与 DataFrame、DataSet 三者的共性"></a>RDD 与 DataFrame、DataSet 三者的共性</h4><ul>
<li>三者都是<strong>分布式弹性数据集</strong>，支持相互转化</li>
<li>三者有<strong>许多共同的函数</strong>，如 filter，排序等</li>
<li>三者<strong>都是 Lazy（惰性操作） 的</strong>，在进行创建、转换时，不会立即执行<ul>
<li>只有在遇到 Action 算子时，才会开始遍历运算</li>
</ul>
</li>
</ul>
<h2 id="Part-3-Spark-原理与架构"><a href="#Part-3-Spark-原理与架构" class="headerlink" title="Part 3 Spark 原理与架构"></a>Part 3 Spark 原理与架构</h2><h3 id="Spark-体系架构"><a href="#Spark-体系架构" class="headerlink" title="Spark 体系架构"></a>Spark 体系架构</h3><p><img src="https://alipicbed.oss-cn-beijing.aliyuncs.com/img/image-20210904113924087.png" srcset="/img/loading.gif" lazyload alt="Spark 体系架构"></p>
<h4 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core"></a>Spark Core</h4><p>类似于 MapReduce 的分布式内存计算框架，最大的特点是<strong>将中间计算结果直接放在内存中</strong>，提升计算性能。</p>
<p>自带了 Standalone 模式的资源管理框架，同时，也支持YARN、 MESOS的资源管理系统。</p>
<p>FI 集成的是 Spark On Yarn 的模式，其它模式暂不支持。</p>
<h4 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h4><p>Spark SQL 是一个用于处理结构化数据的 Spark 组件，作为 Apache  Spark 大数据框架的一部分，主要用于<strong>结构化数据处理和对数据执行类SQL查询</strong>。</p>
<p>通过 Spark SQL，可以针对不同数据格式（如：JSON，Parquet， ORC等）和数据源执行 ETL操作（如：HDFS、数据库等），完成特定的查询操作。</p>
<h4 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h4><p>微批处理的流处理引擎，将流数据分片以后用 Spark Core 的计算引擎 中进行处理。相对于Storm，实时性稍差，优势<strong>体现在吞吐量上</strong></p>
<h4 id="MLlib-和-GraphX"><a href="#MLlib-和-GraphX" class="headerlink" title="MLlib 和 GraphX"></a>MLlib 和 GraphX</h4><p>主要一些算法库（机器学习算法库和图计算算法库）</p>
<h4 id="Structured-Streaming"><a href="#Structured-Streaming" class="headerlink" title="Structured Streaming"></a>Structured Streaming</h4><p>为 2.0 版本之后的 Spark 独有，吸收了一些 Flink 的特性和功能</p>
<h3 id="典型案例-WordCount"><a href="#典型案例-WordCount" class="headerlink" title="典型案例 - WordCount"></a>典型案例 - WordCount</h3><p><img src="https://alipicbed.oss-cn-beijing.aliyuncs.com/img/image-20210904114425809.png" srcset="/img/loading.gif" lazyload alt="WordCount案例"></p>
<p><img src="https://alipicbed.oss-cn-beijing.aliyuncs.com/img/image-20210904114503856.png" srcset="/img/loading.gif" lazyload alt="Scala核心代码"></p>
<h3 id="Spark-SQL-概述"><a href="#Spark-SQL-概述" class="headerlink" title="Spark SQL 概述"></a>Spark SQL 概述</h3><p><img src="https://alipicbed.oss-cn-beijing.aliyuncs.com/img/image-20210904114704510.png" srcset="/img/loading.gif" lazyload alt="Spark SQL"></p>
<p><strong>Spark SQL</strong> 是 <strong>Spark</strong> 中用于<strong>结构化数据处理的模块</strong>。在 <strong>Spark</strong> 应用中，可以无缝地使用 SQL 语句亦或是 <strong>DataFrame API</strong> 对结构化数据进行查询。</p>
<p>Spark SQL 对 SQL 语句的处理和关系型数据库采用了类似的方法，SparkSQL 先会将 SQL 语句进行解析（Parse）形成一个 Tree， 然后使用 Rule 对 Tree 进行绑定、 优化等处理过程</p>
<ul>
<li><strong>词法和语法解析（Parse）</strong><ul>
<li>对读入的 SQL 语句进行词法和语法解析（Parse），分辨出 SQL 语句中那些词是关键词（如 SELECT、FROM、WHERE），哪些是表达式，哪些是 Projection，哪些是 Data Source 等，判断 SQL 语句是否规范，并形成逻辑计划</li>
</ul>
</li>
<li><strong>绑定（Bind）</strong><ul>
<li>将SQL语句和数据库的数据字典（列、 表和视图等）进行绑定(Bind)，如果相关的 Projection 和 Data Source 等都存在的话，则表示这个 SQL 语句是可以执行的</li>
</ul>
</li>
<li><strong>优化（Optimize）</strong><ul>
<li>Spark SQL 会提供几个执行计划，返回从数据库查询的数据集</li>
</ul>
</li>
<li><strong>执行（Execute）</strong><ul>
<li>执行前面步骤获取的最优执行计划，返回从数据库查询的数据集</li>
</ul>
</li>
</ul>
<h3 id="Spark-SQL-和-Hive-对比"><a href="#Spark-SQL-和-Hive-对比" class="headerlink" title="Spark SQL 和 Hive 对比"></a>Spark SQL 和 Hive 对比</h3><p>两者区别在于——</p>
<ul>
<li><strong>Spark SQL</strong> 的执行引擎为 <strong>Spark Core</strong>，<strong>Hive</strong> 默认执行引擎是 <strong>MapReduce</strong></li>
<li><strong>Spark SQL</strong> 的执行速度是 <strong>Hive</strong> 的 10 - 100 倍</li>
<li><strong>Spark SQL</strong> 不支持 <strong>buckets，Hive</strong> 支持</li>
</ul>
<p>两者的联系在于——</p>
<ul>
<li><strong>Spark SQL</strong> 依赖 <strong>Hive</strong> 的元数据</li>
<li><strong>Spark SQL</strong> 兼容绝大部分 <strong>Hive</strong> 的语法和函数</li>
<li><strong>Spark SQL</strong> 可以使用 <strong>Hive</strong> 的自定义函数</li>
</ul>
<p><strong>Spark SQL</strong> 和 <strong>Hive</strong> 的语法除了<strong>桶表操作外</strong>，基本一致，且 <strong>Spark SQL</strong> 完美兼容 <strong>Hive</strong> 的函数。</p>
<h3 id="Structured-Streaming-概述"><a href="#Structured-Streaming-概述" class="headerlink" title="Structured Streaming 概述"></a>Structured Streaming 概述</h3><p><img src="https://alipicbed.oss-cn-beijing.aliyuncs.com/img/image-20210904120551029.png" srcset="/img/loading.gif" lazyload alt="Structured Streaming 概述"></p>
<p><strong>Structured Streaming</strong> 是构建在 <strong>Spark SQL</strong> 引擎上的流式数据处理引擎，可以像使用静态 RDD 数据那样编写流式计算过程。</p>
<p>当流数据连续不断的产生时，<strong>Spark SQL</strong> 将会<strong>增量地、持续不断地处理</strong>这些数据，并将结果更新到结果集中。</p>
<p><strong>Structured Streaming</strong> 的核心是<strong>将流式的数据看成一张数据不断增加的数据库表</strong></p>
<ul>
<li>这种流式的数据处理模型类似于数据块处理模型，可以把静态数据库表的一些查询操作应用在流式计算中，Spark 执行标准的 SQL 查询，从无边界表中获取数据</li>
<li>无边界表：新数据不断到来，旧数据不断丢弃，实际上是一个连续不断的结构化数据流</li>
</ul>
<h4 id="计算模型示例"><a href="#计算模型示例" class="headerlink" title="计算模型示例"></a>计算模型示例</h4><p><img src="https://alipicbed.oss-cn-beijing.aliyuncs.com/img/image-20210904120848499.png" srcset="/img/loading.gif" lazyload alt="Structured Streaming 计算模型示例"></p>
<ul>
<li>第一个 lines DataFrame 对象是一张数据输入的 Input Table，最后的 WordCounts  DataFrame 是一个结果集 Result Table</li>
<li>在 lines DataFrame 数据流之上的查询产生了 WordCounts 的表示方式和在静态的 Static DataFrame 上的使用方式相同。</li>
<li>然而， Spark 会监控 socket 连接，获取新的持续不断产生的数据。</li>
<li>当新的数据产生时，Spark 将会在新数据上运行一个增量的 counts 查询，并且整合新的 counts 和之前已经计算出来的 counts，获取更新后的 counts</li>
</ul>
<h3 id="Spark-Streaming-概述"><a href="#Spark-Streaming-概述" class="headerlink" title="Spark Streaming 概述"></a>Spark Streaming 概述</h3><p><img src="https://alipicbed.oss-cn-beijing.aliyuncs.com/img/image-20210904121518306.png" srcset="/img/loading.gif" lazyload alt="Spark Streaming 概述"></p>
<p><strong>Spark Sreaming</strong> 的基本原理是<strong>将实时输入数据流以时间片（秒级）为单位</strong>进行拆分，然后经 <strong>Spark</strong> 引擎以类似批处理的方式处理每个时间片数据。</p>
<ul>
<li>把输入数据以秒（毫秒）为单位切分，再定时提交这些切分后的数据</li>
</ul>
<h3 id="窗口间隔和滑动间隔"><a href="#窗口间隔和滑动间隔" class="headerlink" title="窗口间隔和滑动间隔"></a>窗口间隔和滑动间隔</h3><p><img src="https://alipicbed.oss-cn-beijing.aliyuncs.com/img/image-20210904121626600.png" srcset="/img/loading.gif" lazyload alt="窗口间隔和滑动间隔"></p>
<p>窗口在 DStream 上华东，合并和操作落入窗口内的 RDDs，产生窗口化的 RDDs</p>
<ul>
<li><strong>窗口长度：</strong>窗口的持续时间</li>
<li><strong>滑动窗口间隔：</strong>窗口操作执行的时间间隔</li>
</ul>
<h3 id="Spark-Streaming-和-Storm-对比"><a href="#Spark-Streaming-和-Storm-对比" class="headerlink" title="Spark Streaming 和 Storm 对比"></a>Spark Streaming 和 Storm 对比</h3><p><img src="https://alipicbed.oss-cn-beijing.aliyuncs.com/img/image-20210904121755263.png" srcset="/img/loading.gif" lazyload alt="Spark Streaming 和 Storm 对比"></p>
<p>事实上，Spark Streaming 绝对谈不上比 Storm 优秀。这两个框架在实时计算领域中，都很优秀，只是擅长的细分场景并不相同。Spark Streaming <strong>仅仅在吞吐量上</strong>比 Storm 要优秀。</p>
<h4 id="对于-Storm-来说"><a href="#对于-Storm-来说" class="headerlink" title="对于 Storm 来说"></a>对于 Storm 来说</h4><ul>
<li>建议在那种需要纯实时，不能忍受1秒以上延迟的场景下使用，比如实时金融系统， 要求纯实时进行金融交易和分析</li>
<li>如果对于实时计算的功能中，<strong>要求可靠的事务机制和可靠性机制</strong>，即<strong>数据的处理完全精准</strong>，一条也不能多，一条也不能少，也可以考虑使用 Storm</li>
<li>如果还需要针对高峰低峰时间段，<strong>动态调整实时计算程序的并行度</strong>，以最大限度利用集群资源（通常是在小型公司，集群资源紧张的情况），也可以考虑用 Storm</li>
<li>如果一个大数据应用系统，它就是<strong>纯粹的实时计算</strong>，不需要在中间执行 SQL 交互式 查询、复杂的 Transformation 算子等，那么用 Storm 是比较好的选择</li>
</ul>
<h4 id="对于Spark-Streaming来说"><a href="#对于Spark-Streaming来说" class="headerlink" title="对于Spark Streaming来说"></a>对于Spark Streaming来说</h4><ul>
<li>如果对上述适用于 Storm 的三点，一条都不满足的实时场景，即不要求纯实时，不要求强大可靠的事务机制，不要求动态调整并行度，那么可以考虑使用 Spark  Streaming</li>
<li>位于 Spark 生态技术栈中，因此 Spark Streaming 可以和 Spark Core、Spark  SQL 无缝整合，也就意味着，我们可以对实时处理出来的中间数据，立即在程序中无缝进行延迟批处理、交互式查询等操作。这个特点大大增强了 Spark Streaming 的优势和功能</li>
</ul>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Big-Data-Notes/">Big Data Notes</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Notes/">Notes</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2021/09/04/LeetCode-Notes-763/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">LeetCode Notes - 763</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/09/03/Big-Data-Intro-Chapter-5/">
                        <span class="hidden-mobile">Big Data Intro - Chapter 5</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js" ></script>



  <script  src="/js/local-search.js" ></script>






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
